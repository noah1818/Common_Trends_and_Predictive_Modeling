{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876c396b",
   "metadata": {},
   "source": [
    "# Replication Notebook\n",
    "\n",
    "This notebook accompanies our paper and demonstrates, step by step, how the proposed methodology reproduces the main empirical findings. In particular, it serves three purposes:\n",
    "\n",
    "1. **Replication of Figures:** \n",
    "   We begin by reproducing Figure 2 from the paper. This provides a visual check that our implementation matches the patterns reported in the published results.  \n",
    "\n",
    "2. **Replication of Tables:**  \n",
    "   Then, we replicate Table 1 and Table 2. These tables report the numerical results that summarize model performance, parameter estimates, and selection accuracy. Reproducing them ensures the pipeline is correct and demonstrates that our code can fully support the empirical analysis in the paper.  \n",
    "\n",
    "---\n",
    "\n",
    "This paper will not go over how the high dimensional and normal VECM algorithm work, for that please see the other (two) notebooks.\n",
    "For example we wont show that all time series are I(1) every time.\n",
    "\n",
    "\n",
    "First we will use the latent efficient bid price from section 4.2 of our paper, and then we will use section 4.3, the testing procedure for table 1 and table 2 is as described in 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25675ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589c557",
   "metadata": {},
   "source": [
    "## Testing the Common Trend Component for Accuracy\n",
    "\n",
    "We will use Section 4.4 first.\n",
    "To evaluate whether the extracted and scaled common trend component \n",
    "$\\{ \\widehat{y}_{t,j}^{(b)} \\}_{t=1}^{T}$ (or the permanent component) captures meaningful structure in the underlying asset,  \n",
    "we study whether the observed price series $\\{ y_{t,j}^{(b)} \\}_{t=1}^{T}$ tends to revert toward it after deviating by more than a relative threshold $\\delta \\in [0,1)$.  \n",
    "\n",
    "Formally, we define a sequence of *crossing times* and then construct a metric that quantifies how often the observed price returns to its latent efficient price following a significant deviation.\n",
    "\n",
    "---\n",
    "\n",
    "### Defining the crossing sequence\n",
    "\n",
    "Let $t = 1,\\dots,T$ and define the sequence of indices $\\{ i_k \\}_{k=0}^{W}$ with $W > 0$, where each $i_k$ is the time of the next “crossing” event. We alternate between downward and upward deviations as follows:\n",
    "\n",
    "- The first index $i_0$ is the earliest time where the observed price falls at least $\\delta$ below its trend:\n",
    "  $$\n",
    "  i_0 = \\inf \\{ k \\in [1,T] : y_{t,k}^{(b)} < \\widehat{y}_{t,k}^{(b)} (1-\\delta) \\}.\n",
    "  $$\n",
    "- The next index $i_1$ is the first time after $i_0$ where the series crosses back above the trend:\n",
    "  $$\n",
    "  i_1 = \\inf \\{ k \\in (i_0,T] : y_{t,k}^{(b)} \\geq \\widehat{y}_{t,k}^{(b)} \\}.\n",
    "  $$\n",
    "- Then $i_2$ is defined analogously as the first time after $i_1$ where the series again drops at least $\\delta$ below the trend, and so on.\n",
    "\n",
    "In general,\n",
    "$$\n",
    "i_k = \\inf \\Bigg\\{ n \\in (i_{k-1},T] :\n",
    "\\begin{cases}\n",
    "y_{t,n}^{(b)} < \\widehat{y}_{t,n}^{(b)} (1-\\delta), & k \\ \\text{even}, \\\\[6pt]\n",
    "y_{t,n}^{(b)} \\geq \\widehat{y}_{t,n}^{(b)}, & k \\ \\text{odd}.\n",
    "\\end{cases}\n",
    "\\Bigg\\}.\n",
    "$$\n",
    "\n",
    "The sequence terminates once $i_k \\in \\{T,\\infty\\}$. In that case, the last stopping time is defined as\n",
    "$$\n",
    "i_N \\coloneqq\n",
    "\\begin{cases}\n",
    "i_{k-1}, & k \\ \\text{even}, \\\\\n",
    "i_k, & k \\ \\text{odd}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "By convention, $\\inf\\{\\emptyset\\} = \\infty$, so the sequence $\\{ i_k \\}$ is always well-defined, even if some conditions are never met.\n",
    "\n",
    "---\n",
    "\n",
    "### Empirical reversion probability\n",
    "\n",
    "We are interested in the conditional probability that the observed price moves upward once it crosses back above the trend after being at least $\\delta$ below. Intuitively, this corresponds to the probability that an undervalued asset reverts toward equilibrium.\n",
    "\n",
    "Formally,\n",
    "$$\n",
    "\\mathbb{P}\\!\\left[\n",
    "y_{i_{k+1},j}^{(b)} > y_{i_{k},j}^{(b)} ,\\ \n",
    "y_{i_{k+1},j}^{(b)} \\geq \\widehat{y}_{i_{k+1},j}^{(b)}\n",
    "\\ \\middle|\\ \n",
    "y_{i_{k},j}^{(b)} < \\widehat{y}_{i_{k},j}^{(b)} (1-\\delta)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "By the definition of the stopping times, the additional conditions  \n",
    "$y_{i_{k+1},j}^{(b)} \\geq \\widehat{y}_{i_{k+1},j}^{(b)}$ and  \n",
    "$y_{i_{k},j}^{(b)} < \\widehat{y}_{i_{k},j}^{(b)} (1-\\delta)$  \n",
    "are already satisfied, so this reduces to\n",
    "$$\n",
    "\\mathbb{P}\\!\\left[ y_{i_{k+1},j}^{(b)} > y_{i_{k},j}^{(b)} \\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Estimator\n",
    "\n",
    "The empirical counterpart is\n",
    "\n",
    "$$\n",
    "\\widehat{P}_{\\uparrow}\n",
    "= \\frac{1}{N} \\sum_{k=0}^{N-1}\n",
    "\\mathbf{1}\\!\\left\\{\\, y_{i_{k+1},j}^{(b)} > y_{i_{k},j}^{(b)} \\,\\right\\},\n",
    "$$\n",
    "\n",
    "where  \n",
    "\n",
    "- $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function,  \n",
    "- $i_k$ and $i_{k+1}$ are consecutive crossing times (down $\\to$ up),  \n",
    "- $N$ is the total number of such events before termination.  \n",
    "\n",
    "This statistic measures the fraction of downward deviations that are followed by an upward correction, quantifying the predictive content of the estimated common trend.\n",
    "\n",
    "---\n",
    "\n",
    "### Downward counterpart\n",
    "\n",
    "Analogously, define $\\widehat{P}_{\\downarrow}$ for upward deviations beyond $(1+\\delta)$ of the trend, which measures the frequency with which prices correct downward after significant positive departures.\n",
    "\n",
    "Trivially we will use TP, TN, FP and FN to denote the probabilitys of reversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47082ee4",
   "metadata": {},
   "source": [
    "## Standard approach (3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from group_lasso_prox_lag import GroupLassoProxLagSelection\n",
    "from group_lasso_prox_rank import GroupLassoProxRankSelection\n",
    "from helpers import Helpers\n",
    "from multinomial_logit_model import MultinomialLogit\n",
    "from vecm_model_high_dim import VECMModelHD\n",
    "from vecm_model_standard import VECMModel\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import HessianInversionWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", HessianInversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exchange names as hinted in footnote 1 of our paper\n",
    "DF_NAMES = ['binance', 'bitmart', 'bybit', 'hashkey', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "DF_NAMES_CLEAN = ['binance', 'bitmart', 'bybit', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "\n",
    "# Keep track of the performance of every exchange respectively\n",
    "score = {}\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    score[exchange_name] = {}\n",
    "    score[exchange_name]['BID_SIDE'] = {}\n",
    "    score[exchange_name]['BID_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FN'] = 0\n",
    "\n",
    "    score[exchange_name]['ASK_SIDE'] = {}\n",
    "    score[exchange_name]['ASK_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FN'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reproduce Figure 2, one needs to load the wirst 50_000 rows on the csv file and use the following plotting method:\n",
    "\n",
    "'''plt.plot(Y_b[0][:220], label='binance bids')\n",
    "plt.plot(Y_b[1][:220], label='bitmart bids')\n",
    "plt.plot(Y_b[2][:220], label='bybit bids')\n",
    "plt.plot(Y_b[3][:220], label='htx bids')\n",
    "plt.plot(Y_b[4][:220], label='bitget bids')\n",
    "plt.plot(Y_b[5][:220], label='deribit bids')\n",
    "plt.plot(Y_b[6][:220], label='kuma bids')\n",
    "plt.plot(Y_b[7][:220], label='hyperliquid bids')\n",
    "\n",
    "plt.xlabel(\"t\")        # x-axis label\n",
    "plt.ylabel(\"price\")    # y-axis label\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce742fe8",
   "metadata": {},
   "source": [
    "### Using Section 4.2 (Granger Representation Theorem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e65572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size\n",
    "H = 50_000\n",
    "# Train percentage\n",
    "RHO = 0.7\n",
    "# Max total length of the data we look at\n",
    "max_j = 11964000 # pre determined number of rows of the file\n",
    "# The stepsize\n",
    "STEPSIZE = int((1-RHO)*H)\n",
    "# Set the lag once\n",
    "LAG = 20\n",
    "# Set the rank once\n",
    "RANK = 7 # K - 1\n",
    "# Set threshold delta\n",
    "DELTA = 0.0005\n",
    "# Path\n",
    "PATH = \"/Downloads/data.csv\"\n",
    "\n",
    "for j in range(0, max_j, STEPSIZE):\n",
    "    try:\n",
    "        # This should be the path to the data_1.csv file on your local machine\n",
    "        skip_rows = j\n",
    "        n_rows = H\n",
    "\n",
    "        VECM_model_b = VECMModel()\n",
    "        VECM_model_a = VECMModel()\n",
    "        helper_methods = Helpers()\n",
    "\n",
    "        binance, bitmart, bybit, hashkey, htx, bitget, deribit, kuma, hyperliquid = helper_methods.read_csv_to_dataframe(PATH, skip_rows, n_rows, DF_NAMES)\n",
    "        # We will skip hashkey exchange\n",
    "        Y_b = VECM_model_b.construct_Y([binance.bids, bitmart.bids, bybit.bids, htx.bids, bitget.bids, deribit.bids, kuma.bids, hyperliquid.bids])\n",
    "        Y_a = VECM_model_a.construct_Y([binance.asks, bitmart.asks, bybit.asks, htx.asks, bitget.asks, deribit.asks, kuma.asks, hyperliquid.asks])\n",
    "        # Train test split\n",
    "        Y_train_b, Y_test_b = VECM_model_b.train_test_split_Y(Y_b, RHO)\n",
    "        Y_train_a, Y_test_a = VECM_model_a.train_test_split_Y(Y_a, RHO)\n",
    "        # Set the lag\n",
    "        VECM_model_b.lag = LAG\n",
    "        VECM_model_a.lag = LAG\n",
    "        # Build the VECM matrices\n",
    "        VECM_model_b.build_vecm_matrices(Y_train_b)\n",
    "        VECM_model_a.build_vecm_matrices(Y_train_a)\n",
    "        # Build residual covaraince matrices\n",
    "        VECM_model_b.build_residual_covariances()\n",
    "        VECM_model_a.build_residual_covariances()\n",
    "        # Build johansen matrix\n",
    "        S_tilde_b = VECM_model_b.get_johansen_matrix()\n",
    "        S_tilde_a = VECM_model_a.get_johansen_matrix()\n",
    "        # Compute sorted eigenvalues\n",
    "        eigvals_sorted_b, _ = VECM_model_b.sort_eigenvectors(S_tilde_b)\n",
    "        eigvals_sorted_a, _ = VECM_model_a.sort_eigenvectors(S_tilde_a)\n",
    "        # Set Rank\n",
    "        VECM_model_b.rank = RANK\n",
    "        VECM_model_a.rank = RANK\n",
    "        # Compute the variables\n",
    "        alpha_b, beta_b, Gammas_b, Sigma_u_b = VECM_model_b.vecm_variable_estimation()\n",
    "        alpha_a, beta_a, Gammas_a, Sigma_u_a = VECM_model_a.vecm_variable_estimation()\n",
    "        # Compute the orthogonal compoents of alpha and beta\n",
    "        alpha_perp_b = VECM_model_b.compute_null_space_basis(alpha_b.T)\n",
    "        beta_perp_b = VECM_model_b.compute_null_space_basis(beta_b.T)\n",
    "        alpha_perp_a = VECM_model_a.compute_null_space_basis(alpha_a.T)\n",
    "        beta_perp_a = VECM_model_a.compute_null_space_basis(beta_a.T)\n",
    "        # First evaluate the latent efficient price from Section 4.2 of our paper\n",
    "        Xi_b = VECM_model_b.compute_granger_representation_matrix_XI(alpha_perp_b, beta_perp_b, Gammas_b)\n",
    "        Xi_a = VECM_model_a.compute_granger_representation_matrix_XI(alpha_perp_a, beta_perp_a, Gammas_a)\n",
    "\n",
    "        # Loop over all variables to compute the score respectively\n",
    "        for idx, exchange_name in zip(range(len(DF_NAMES_CLEAN)), DF_NAMES_CLEAN):\n",
    "            Lambda_b = (Xi_b @ Y_train_b)[idx]\n",
    "            Lambda_a = (Xi_b @ Y_train_b)[idx]\n",
    "\n",
    "            y_b = Y_train_b[idx].T\n",
    "            y_breve_b = Lambda_b\n",
    "\n",
    "            y_a = Y_train_a[idx].T\n",
    "            y_breve_a = Lambda_a\n",
    "            # Compute scaling factor\n",
    "            phi_b = VECM_model_b.estimate_phi(y_b, y_breve_b)\n",
    "            phi_a = VECM_model_a.estimate_phi(y_a, y_breve_a)\n",
    "            # Define efficient latent prices\n",
    "            efficient_price_b = phi_b * (Xi_b @ Y_test_b)[idx]\n",
    "            efficient_price_a = phi_a * (Xi_b @ Y_test_a)[idx]\n",
    "\n",
    "            Y_test_b_idx = Y_test_b[idx]\n",
    "            Y_test_a_idx = Y_test_a[idx]\n",
    "            # Performance for bid series\n",
    "            crossed_below_delta = np.where(Y_test_b_idx < efficient_price_b * (1 - DELTA))[0]\n",
    "            crossed_above = np.where(Y_test_b_idx > efficient_price_b)[0]\n",
    "\n",
    "            crossed_above_delta = np.where(Y_test_b_idx> efficient_price_b * (1 + DELTA))[0]\n",
    "            crossed_below = np.where(Y_test_b_idx < efficient_price_b)[0]\n",
    "\n",
    "            for i_k in crossed_below_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_above[np.where(crossed_above > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_b_idx[i_k_1] - Y_test_b_idx[i_k]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['BID_SIDE']['TP'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FP'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            for i_k in crossed_above_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_below[np.where(crossed_below > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_b_idx[i_k] - Y_test_b_idx[i_k_1]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['BID_SIDE']['TN'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FN'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            # Performance for ask series\n",
    "            crossed_below_delta = np.where(Y_test_a_idx < efficient_price_a * (1 - DELTA))[0]\n",
    "            crossed_above = np.where(Y_test_a_idx > efficient_price_a)[0]\n",
    "\n",
    "            crossed_above_delta = np.where(Y_test_a_idx > efficient_price_a * (1 + DELTA))[0]\n",
    "            crossed_below = np.where(Y_test_a_idx < efficient_price_a)[0]\n",
    "\n",
    "            for i_k in crossed_below_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_above[np.where(crossed_above > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_a_idx[i_k_1] - Y_test_a_idx[i_k]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['TP'] += 1\n",
    "                    elif difference < 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['FP'] += 1\n",
    "                \n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            for i_k in crossed_above_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_below[np.where(crossed_below > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_a_idx[i_k] - Y_test_a_idx[i_k_1]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['TN'] += 1\n",
    "                    elif difference < 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['FN'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        print(f\"###{j / max_j}###\")\n",
    "\n",
    "    except Exception as exp:\n",
    "        print(f\"Something went wrong: {exp}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentages of the score\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    accuracy_bid_side_up = score[exchange_name]['BID_SIDE']['TP'] / (score[exchange_name]['BID_SIDE']['TP'] + score[exchange_name]['BID_SIDE']['FP'])\n",
    "    accuracy_ask_side_up = score[exchange_name]['ASK_SIDE']['TP'] / (score[exchange_name]['ASK_SIDE']['TP'] + score[exchange_name]['ASK_SIDE']['FP'])\n",
    "\n",
    "    accuracy_bid_side_down = score[exchange_name]['BID_SIDE']['TN'] / (score[exchange_name]['BID_SIDE']['TN'] + score[exchange_name]['BID_SIDE']['FN'])\n",
    "    accuracy_ask_side_down = score[exchange_name]['ASK_SIDE']['TN'] / (score[exchange_name]['ASK_SIDE']['TN'] + score[exchange_name]['ASK_SIDE']['FN'])\n",
    "\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_bid_side_up} for updwards correction on the bid side\")\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_ask_side_up} for downwards correction on the ask side\")\n",
    "\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_bid_side_down} for updwards correction on the bid side\")\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_ask_side_down} for downwards correction on the ask side\")\n",
    "    print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ba544",
   "metadata": {},
   "source": [
    "### Using Section 4.3 (P-T Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91155be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exchange names as hinted in footnote 1 of our paper\n",
    "DF_NAMES = ['binance', 'bitmart', 'bybit', 'hashkey', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "DF_NAMES_CLEAN = ['binance', 'bitmart', 'bybit', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "\n",
    "# Keep track of the performance of every exchange respectively\n",
    "score = {}\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    score[exchange_name] = {}\n",
    "    score[exchange_name]['BID_SIDE'] = {}\n",
    "    score[exchange_name]['BID_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FN'] = 0\n",
    "\n",
    "    score[exchange_name]['ASK_SIDE'] = {}\n",
    "    score[exchange_name]['ASK_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FN'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size\n",
    "H = 50_000\n",
    "# Train percentage\n",
    "RHO = 0.7\n",
    "# Max total length of the data we look at\n",
    "max_j = 11964000 # pre determined number of rows of the file\n",
    "# The stepsize\n",
    "STEPSIZE = int((1-RHO)*H)\n",
    "# Set the lag once\n",
    "LAG = 20\n",
    "# Set the rank once\n",
    "RANK = 7 # K - 1\n",
    "# Set threshold delta\n",
    "DELTA = 0.0005\n",
    "# Path to the data\n",
    "PATH = \"/Downloads/data.csv\"\n",
    "\n",
    "for j in range(0, max_j, STEPSIZE):\n",
    "    try:\n",
    "        # This should be the path to the data_1.csv file on your local machine\n",
    "        skip_rows = j\n",
    "        n_rows = H\n",
    "\n",
    "        VECM_model_b = VECMModel()\n",
    "        VECM_model_a = VECMModel()\n",
    "        helper_methods = Helpers()\n",
    "\n",
    "        binance, bitmart, bybit, hashkey, htx, bitget, deribit, kuma, hyperliquid = helper_methods.read_csv_to_dataframe(PATH, skip_rows, n_rows, DF_NAMES)\n",
    "        Y_b = VECM_model_b.construct_Y([binance.bids, bitmart.bids, bybit.bids, htx.bids, bitget.bids, deribit.bids, kuma.bids, hyperliquid.bids])\n",
    "        Y_a = VECM_model_a.construct_Y([binance.asks, bitmart.asks, bybit.asks, htx.asks, bitget.asks, deribit.asks, kuma.asks, hyperliquid.asks])\n",
    "        # Train test split\n",
    "        Y_train_b, Y_test_b = VECM_model_b.train_test_split_Y(Y_b, RHO)\n",
    "        Y_train_a, Y_test_a = VECM_model_a.train_test_split_Y(Y_a, RHO)\n",
    "        # Set the lag\n",
    "        VECM_model_b.lag = LAG\n",
    "        VECM_model_a.lag = LAG\n",
    "        # Build the VECM matrices\n",
    "        VECM_model_b.build_vecm_matrices(Y_train_b)\n",
    "        VECM_model_a.build_vecm_matrices(Y_train_a)\n",
    "        # Build residual covaraince matrices\n",
    "        VECM_model_b.build_residual_covariances()\n",
    "        VECM_model_a.build_residual_covariances()\n",
    "        # Build johansen matrix\n",
    "        S_tilde_b = VECM_model_b.get_johansen_matrix()\n",
    "        S_tilde_a = VECM_model_a.get_johansen_matrix()\n",
    "        # Compute sorted eigenvalues\n",
    "        eigvals_sorted_b, _ = VECM_model_b.sort_eigenvectors(S_tilde_b)\n",
    "        eigvals_sorted_a, _ = VECM_model_a.sort_eigenvectors(S_tilde_a)\n",
    "        # Set Rank\n",
    "        VECM_model_b.rank = RANK\n",
    "        VECM_model_a.rank = RANK\n",
    "        # Compute the variables\n",
    "        alpha_b, beta_b, Gammas_b, Sigma_u_b = VECM_model_b.vecm_variable_estimation()\n",
    "        alpha_a, beta_a, Gammas_a, Sigma_u_a = VECM_model_a.vecm_variable_estimation()\n",
    "        # Compute the orthogonal compoents of alpha and beta\n",
    "        alpha_perp_b = VECM_model_b.compute_null_space_basis(alpha_b.T)\n",
    "        beta_perp_b = VECM_model_b.compute_null_space_basis(beta_b.T)\n",
    "        alpha_perp_a = VECM_model_a.compute_null_space_basis(alpha_a.T)\n",
    "        beta_perp_a = VECM_model_a.compute_null_space_basis(beta_a.T)\n",
    "        # First evaluate the latent efficient price from Section 4.2 of our paper\n",
    "        persistent_component_b, transitory_component_b = VECM_model_b.compute_P_T_decomposition(alpha_b, beta_b, alpha_perp_b, beta_perp_b, Y_test_b)\n",
    "        persistent_component_a, transitory_component_a = VECM_model_a.compute_P_T_decomposition(alpha_b, beta_b, alpha_perp_a, beta_perp_a, Y_test_a)\n",
    "\n",
    "        # Loop over all variables to compute the score respectively\n",
    "        for idx, exchange_name in zip(range(len(DF_NAMES_CLEAN)), DF_NAMES_CLEAN):\n",
    "            # Define efficient latent prices\n",
    "            efficient_price_b = persistent_component_b[idx]\n",
    "            efficient_price_a = persistent_component_a[idx]\n",
    "\n",
    "            Y_test_b_idx = Y_test_b[idx]\n",
    "            Y_test_a_idx = Y_test_a[idx]\n",
    "            # Performance for bid series\n",
    "            crossed_below_delta = np.where(Y_test_b_idx < efficient_price_b * (1 - DELTA))[0]\n",
    "            crossed_above = np.where(Y_test_b_idx > efficient_price_b)[0]\n",
    "\n",
    "            crossed_above_delta = np.where(Y_test_b_idx > efficient_price_b * (1 + DELTA))[0]\n",
    "            crossed_below = np.where(Y_test_b_idx < efficient_price_b)[0]\n",
    "\n",
    "            for i_k in crossed_below_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_above[np.where(crossed_above > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_b_idx[i_k_1] - Y_test_b_idx[i_k]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['BID_SIDE']['TP'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FP'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            for i_k in crossed_above_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_below[np.where(crossed_below > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_b_idx[i_k] - Y_test_b_idx[i_k_1]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['BID_SIDE']['TN'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FN'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            # Performance for ask series\n",
    "            crossed_below_delta = np.where(Y_test_a_idx < efficient_price_a * (1 - DELTA))[0]\n",
    "            crossed_above = np.where(Y_test_a_idx > efficient_price_a)[0]\n",
    "\n",
    "            crossed_above_delta = np.where(Y_test_a_idx > efficient_price_a * (1 + DELTA))[0]\n",
    "            crossed_below = np.where(Y_test_a_idx < efficient_price_a)[0]\n",
    "\n",
    "            for i_k in crossed_below_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_above[np.where(crossed_above > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_a_idx[i_k_1] - Y_test_a_idx[i_k]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['TP'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['ASK_SIDE']['FP'] += 1\n",
    "                \n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            for i_k in crossed_above_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_below[np.where(crossed_below > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_a_idx[i_k] - Y_test_a_idx[i_k_1]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['TN'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['ASK_SIDE']['FN'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        print(f\"###{j / max_j}###\")\n",
    "    \n",
    "    except Exception as exp:\n",
    "        print(f\"Something went wrong: {exp}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentages of the score\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    accuracy_bid_side_up = score[exchange_name]['BID_SIDE']['TP'] / (score[exchange_name]['BID_SIDE']['TP'] + score[exchange_name]['BID_SIDE']['FP'])\n",
    "    accuracy_ask_side_up = score[exchange_name]['ASK_SIDE']['TP'] / (score[exchange_name]['ASK_SIDE']['TP'] + score[exchange_name]['ASK_SIDE']['FP'])\n",
    "\n",
    "    accuracy_bid_side_down = score[exchange_name]['BID_SIDE']['TN'] / (score[exchange_name]['BID_SIDE']['TN'] + score[exchange_name]['BID_SIDE']['FN'])\n",
    "    accuracy_ask_side_down = score[exchange_name]['ASK_SIDE']['TN'] / (score[exchange_name]['ASK_SIDE']['TN'] + score[exchange_name]['ASK_SIDE']['FN'])\n",
    "\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_bid_side_up} for updwards correction on the bid side\")\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_ask_side_up} for downwards correction on the ask side\")\n",
    "\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_bid_side_down} for updwards correction on the bid side\")\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_ask_side_down} for downwards correction on the ask side\")\n",
    "    print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe30ff",
   "metadata": {},
   "source": [
    "## High Dimensional approach (3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f737650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exchange names as hinted in footnote 1 of our paper\n",
    "DF_NAMES = ['binance', 'bitmart', 'bybit', 'hashkey', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "DF_NAMES_CLEAN = ['binance', 'bitmart', 'bybit', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "\n",
    "# Keep track of the performance of every exchange respectively\n",
    "score = {}\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    score[exchange_name] = {}\n",
    "    score[exchange_name]['BID_SIDE'] = {}\n",
    "    score[exchange_name]['BID_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FN'] = 0\n",
    "\n",
    "    score[exchange_name]['ASK_SIDE'] = {}\n",
    "    score[exchange_name]['ASK_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FN'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d1bb3",
   "metadata": {},
   "source": [
    "### Using Section 4.2 (Granger Representation Theorem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size\n",
    "H = 50_000\n",
    "# Train percentage\n",
    "RHO = 0.7\n",
    "# Max total length of the data we look at\n",
    "max_j = 11964000 # pre determined number of rows of the file\n",
    "# The stepsize\n",
    "STEPSIZE = int((1-RHO)*H)\n",
    "# Set the lag once\n",
    "LAG = 20\n",
    "# Set the rank once\n",
    "RANK = 7 # K - 1\n",
    "# Set threshold delta\n",
    "DELTA = 0.0005\n",
    "# Path\n",
    "PATH = \"/Downloads/data.csv\"\n",
    "# Adaptive group lasso solvers\n",
    "group_lasso_prox_rank_b = GroupLassoProxRankSelection()\n",
    "group_lasso_prox_rank_a = GroupLassoProxRankSelection()\n",
    "group_lasso_prox_b = GroupLassoProxLagSelection()\n",
    "group_lasso_prox_a = GroupLassoProxLagSelection()\n",
    "# Define tau rank, tau lag and tau ridge and gamma\n",
    "tau_rank_b = 1e10\n",
    "tau_rank_a = 1e10\n",
    "tau_lag_b = 0.01\n",
    "tau_lag_a = 0.01\n",
    "gamma = 2\n",
    "\n",
    "for j in range(int(0.40496489468405217 * 11964000), max_j, STEPSIZE):\n",
    "    try:\n",
    "        # This should be the path to the data_1.csv file on your local machine\n",
    "        skip_rows = j\n",
    "        n_rows = H\n",
    "\n",
    "        VECM_model_b = VECMModelHD()\n",
    "        VECM_model_a = VECMModelHD()\n",
    "        helper_methods = Helpers()\n",
    "\n",
    "        binance, bitmart, bybit, hashkey, htx, bitget, deribit, kuma, hyperliquid = helper_methods.read_csv_to_dataframe(PATH, skip_rows, n_rows, DF_NAMES)\n",
    "        # We will skip hashkey exchange\n",
    "        Y_b = VECM_model_b.construct_Y([binance.bids, bitmart.bids, bybit.bids, htx.bids, bitget.bids, deribit.bids, kuma.bids, hyperliquid.bids])\n",
    "        Y_a = VECM_model_a.construct_Y([binance.asks, bitmart.asks, bybit.asks, htx.asks, bitget.asks, deribit.asks, kuma.asks, hyperliquid.asks])\n",
    "        # Train test split\n",
    "        Y_train_b, Y_test_b = VECM_model_b.train_test_split_Y(Y_b, RHO)\n",
    "        Y_train_a, Y_test_a = VECM_model_a.train_test_split_Y(Y_a, RHO)\n",
    "        # Set the lag\n",
    "        VECM_model_b.lag = LAG\n",
    "        VECM_model_a.lag = LAG\n",
    "        # Build the VECM matrices\n",
    "        VECM_model_b.build_vecm_matrices(Y_train_b)\n",
    "        VECM_model_a.build_vecm_matrices(Y_train_a)\n",
    "        # Build frisch waugh rank matrices\n",
    "        dY_tilde_b, Y1_tilde_b, Pi_tilde_b = VECM_model_b.frisch_waugh_rank_matrices()\n",
    "        dY_tilde_a, Y1_tilde_a, Pi_tilde_a = VECM_model_a.frisch_waugh_rank_matrices()\n",
    "\n",
    "        # Compute qr decomp\n",
    "        Q_hat_b, R_tilde_b, Perm_b = VECM_model_b.qr_decomp(Pi_tilde_b.T)\n",
    "        Q_hat_a, R_tilde_a, Perm_a = VECM_model_a.qr_decomp(Pi_tilde_a.T)\n",
    "        # Compute permuation matrices\n",
    "        Pmat_b = np.eye(Perm_b.shape[0])[:, Perm_b]\n",
    "        Pmat_a = np.eye(Perm_a.shape[0])[:, Perm_a]\n",
    "        # Compute mu_tildes\n",
    "        mu_tilde_b = VECM_model_b.compute_mu_tilde(R_tilde_b)\n",
    "        mu_tilde_a = VECM_model_a.compute_mu_tilde(R_tilde_a)\n",
    "        # Rank pre estimate\n",
    "        r_hat_qr_decomp_b = VECM_model_b.rank_pre_estimate(mu_tilde_b)\n",
    "        r_hat_qr_decomp_a = VECM_model_a.rank_pre_estimate(mu_tilde_a)\n",
    "        # Betas\n",
    "        beta_b = VECM_model_b.get_beta(Q_hat_b, r_hat_qr_decomp_b)\n",
    "        beta_a = VECM_model_a.get_beta(Q_hat_a, r_hat_qr_decomp_a)\n",
    "        # Regressor matrices\n",
    "        dY_tilde_b = Pmat_b.T @ dY_tilde_b\n",
    "        dY_tilde_a = Pmat_a.T @ dY_tilde_a\n",
    "\n",
    "        Y1_tilde_b = Pmat_b.T @ Y1_tilde_b\n",
    "        Y1_tilde_a = Pmat_a.T @ Y1_tilde_a\n",
    "\n",
    "        T = VECM_model_b.T_eff\n",
    "        K = VECM_model_b.K\n",
    "\n",
    "        weights = group_lasso_prox_rank_b.compute_weights(mu_tilde_b, gamma)\n",
    "        weights = group_lasso_prox_rank_b.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_rank_b.construct_Y_Z_matrices(Q_hat_b, Y1_tilde_b, dY_tilde_b)\n",
    "\n",
    "        R_hat_b, loss_history = group_lasso_prox_rank_b.fit(R_tilde_b, Y, Z, tau_rank_b, weights, tol = 1e-20)\n",
    "\n",
    "        Sigma_w_b = VECM_model_b.residual_covariance(dY_tilde_b, R_hat_b.T @ Q_hat_b.T @ Y1_tilde_b)\n",
    "\n",
    "        group_lasso_prox_rank_a = GroupLassoProxRankSelection()\n",
    "        weights = group_lasso_prox_rank_a.compute_weights(mu_tilde_a, gamma)\n",
    "        weights = group_lasso_prox_rank_a.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_rank_a.construct_Y_Z_matrices(Q_hat_a, Y1_tilde_a, dY_tilde_a)\n",
    "        R_hat_a, loss_history = group_lasso_prox_rank_a.fit(R_tilde_a, Y, Z, tau_rank_a, weights, tol = 1e-20)\n",
    "\n",
    "        Sigma_w_a = VECM_model_a.residual_covariance(dY_tilde_a, R_hat_a.T @ Q_hat_a.T @ Y1_tilde_a)\n",
    "\n",
    "        # Set rank\n",
    "        VECM_model_b.rank = RANK\n",
    "        VECM_model_a.rank = RANK\n",
    "        # Get alphas\n",
    "        alpha_b = VECM_model_b.get_alpha(Pmat_b, R_hat_b, r_hat_qr_decomp_b)\n",
    "        alpha_a = VECM_model_a.get_alpha(Pmat_a, R_hat_a, r_hat_qr_decomp_a)\n",
    "        # Frisch waugh lag matrices\n",
    "        dY_check_b, dX_check_b = VECM_model_b.frisch_waugh_lag_matrices()\n",
    "        dY_check_a, dX_check_a = VECM_model_a.frisch_waugh_lag_matrices()\n",
    "        # Gammas pre estimates\n",
    "        Gamma_check_b = VECM_model_b.least_squares_estimate_Gamma(dX_check_b, dY_check_b)\n",
    "        Gamma_check_a = VECM_model_a.least_squares_estimate_Gamma(dX_check_a, dY_check_a)\n",
    "        c_b = 1\n",
    "        c_a = 1\n",
    "        tau_ridge_b = c_b * np.log(T)\n",
    "        tau_ridge_a = c_a * np.log(T)\n",
    "        Gamma_tilde_b = VECM_model_b.ridge_estimator_Gamma(dX_check_b, dY_check_b, tau_ridge_b, LAG)\n",
    "        Gamma_tilde_a = VECM_model_a.ridge_estimator_Gamma(dX_check_a, dY_check_a, tau_ridge_a, LAG)\n",
    "\n",
    "        # lag estimation\n",
    "        K = VECM_model_b.K\n",
    "        weights = group_lasso_prox_b.compute_weights(Gamma_tilde_b, LAG, K, gamma)\n",
    "        weights = group_lasso_prox_b.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_b.helper_loss(LAG, dY_check_b)\n",
    "        Gamma_hat_b, loss_history = group_lasso_prox_b.fit(Gamma_tilde_b, Y, Z, LAG, K, tau_lag_b, weights)\n",
    "        Sigma_w_b = VECM_model_b.residual_covariance(dY_check_b, Gamma_hat_b @ dX_check_b)\n",
    "\n",
    "\n",
    "        weights = group_lasso_prox_a.compute_weights(Gamma_tilde_a, LAG, K, gamma)\n",
    "        weights = group_lasso_prox_a.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_a.helper_loss(LAG, dY_check_a)\n",
    "        Gamma_hat_a, loss_history = group_lasso_prox_a.fit(Gamma_tilde_a, Y, Z, LAG, K, tau_lag_a, weights)\n",
    "        Sigma_w_a = VECM_model_a.residual_covariance(dY_check_a, Gamma_hat_a @ dX_check_a)\n",
    "\n",
    "        # set gamma and lag\n",
    "        Gammas_b = Gamma_hat_b[:, :VECM_model_b.lag*K]\n",
    "        Gammas_a = Gamma_hat_a[:, :VECM_model_a.lag*K]\n",
    "\n",
    "        VECM_model_b.lag = LAG\n",
    "        VECM_model_a.lag = LAG\n",
    "\n",
    "        # Compute the orthogonal compoents of alpha and beta\n",
    "        alpha_perp_b = VECM_model_b.compute_null_space_basis(alpha_b.T)\n",
    "        beta_perp_b = VECM_model_b.compute_null_space_basis(beta_b.T)\n",
    "        alpha_perp_a = VECM_model_a.compute_null_space_basis(alpha_a.T)\n",
    "        beta_perp_a = VECM_model_a.compute_null_space_basis(beta_a.T)\n",
    "        # First evaluate the latent efficient price from Section 4.2 of our paper\n",
    "        Xi_b = VECM_model_b.compute_granger_representation_matrix_XI(alpha_perp_b, beta_perp_b, Gammas_b)\n",
    "        Xi_a = VECM_model_a.compute_granger_representation_matrix_XI(alpha_perp_a, beta_perp_a, Gammas_a)\n",
    "\n",
    "        # Loop over all variables to compute the score respectively\n",
    "        for idx, exchange_name in zip(range(len(DF_NAMES_CLEAN)), DF_NAMES_CLEAN):\n",
    "            Lambda_b = (Xi_b @ Y_train_b)[idx]\n",
    "            Lambda_a = (Xi_b @ Y_train_b)[idx]\n",
    "\n",
    "            y_b = Y_train_b[idx].T\n",
    "            y_breve_b = Lambda_b\n",
    "\n",
    "            y_a = Y_train_a[idx].T\n",
    "            y_breve_a = Lambda_a\n",
    "            # Compute scaling factor\n",
    "            phi_b = VECM_model_b.estimate_phi(y_b, y_breve_b)\n",
    "            phi_a = VECM_model_a.estimate_phi(y_a, y_breve_a)\n",
    "            # Define efficient latent prices\n",
    "            efficient_price_b = phi_b * (Xi_b @ Y_test_b)[idx]\n",
    "            efficient_price_a = phi_a * (Xi_b @ Y_test_a)[idx]\n",
    "\n",
    "            Y_test_b_idx = Y_test_b[idx]\n",
    "            Y_test_a_idx = Y_test_a[idx]\n",
    "            # Performance for bid series\n",
    "            crossed_below_delta = np.where(Y_test_b_idx < efficient_price_b * (1 - DELTA))[0]\n",
    "            crossed_above = np.where(Y_test_b_idx > efficient_price_b)[0]\n",
    "\n",
    "            crossed_above_delta = np.where(Y_test_b_idx> efficient_price_b * (1 + DELTA))[0]\n",
    "            crossed_below = np.where(Y_test_b_idx < efficient_price_b)[0]\n",
    "\n",
    "            for i_k in crossed_below_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_above[np.where(crossed_above > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_b_idx[i_k_1] - Y_test_b_idx[i_k]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['BID_SIDE']['TP'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FP'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            for i_k in crossed_above_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_below[np.where(crossed_below > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_b_idx[i_k] - Y_test_b_idx[i_k_1]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['BID_SIDE']['TN'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FN'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            # Performance for ask series\n",
    "            crossed_below_delta = np.where(Y_test_a_idx < efficient_price_a * (1 - DELTA))[0]\n",
    "            crossed_above = np.where(Y_test_a_idx > efficient_price_a)[0]\n",
    "\n",
    "            crossed_above_delta = np.where(Y_test_a_idx > efficient_price_a * (1 + DELTA))[0]\n",
    "            crossed_below = np.where(Y_test_a_idx < efficient_price_a)[0]\n",
    "\n",
    "            for i_k in crossed_below_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_above[np.where(crossed_above > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_a_idx[i_k_1] - Y_test_a_idx[i_k]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['TP'] += 1\n",
    "                    elif difference < 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['FP'] += 1\n",
    "                \n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            for i_k in crossed_above_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_below[np.where(crossed_below > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_a_idx[i_k] - Y_test_a_idx[i_k_1]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['TN'] += 1\n",
    "                    elif difference < 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['FN'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        print(f\"###{j / max_j}###\")\n",
    "\n",
    "    except Exception as exp:\n",
    "        print(f\"Something went wrong: {exp}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentages of the score\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    accuracy_bid_side_up = score[exchange_name]['BID_SIDE']['TP'] / (score[exchange_name]['BID_SIDE']['TP'] + score[exchange_name]['BID_SIDE']['FP'])\n",
    "    accuracy_ask_side_up = score[exchange_name]['ASK_SIDE']['TP'] / (score[exchange_name]['ASK_SIDE']['TP'] + score[exchange_name]['ASK_SIDE']['FP'])\n",
    "\n",
    "    accuracy_bid_side_down = score[exchange_name]['BID_SIDE']['TN'] / (score[exchange_name]['BID_SIDE']['TN'] + score[exchange_name]['BID_SIDE']['FN'])\n",
    "    accuracy_ask_side_down = score[exchange_name]['ASK_SIDE']['TN'] / (score[exchange_name]['ASK_SIDE']['TN'] + score[exchange_name]['ASK_SIDE']['FN'])\n",
    "\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_bid_side_up} for updwards correction on the bid side\")\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_ask_side_up} for downwards correction on the ask side\")\n",
    "\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_bid_side_down} for updwards correction on the bid side\")\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_ask_side_down} for downwards correction on the ask side\")\n",
    "    print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f2c9e",
   "metadata": {},
   "source": [
    "### Using Section 4.3 (P-T Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exchange names as hinted in footnote 1 of our paper\n",
    "DF_NAMES = ['binance', 'bitmart', 'bybit', 'hashkey', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "DF_NAMES_CLEAN = ['binance', 'bitmart', 'bybit', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "\n",
    "# Keep track of the performance of every exchange respectively\n",
    "score = {}\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    score[exchange_name] = {}\n",
    "    score[exchange_name]['BID_SIDE'] = {}\n",
    "    score[exchange_name]['BID_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FN'] = 0\n",
    "\n",
    "    score[exchange_name]['ASK_SIDE'] = {}\n",
    "    score[exchange_name]['ASK_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FN'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size\n",
    "H = 50_000\n",
    "# Train percentage\n",
    "RHO = 0.7\n",
    "# Max total length of the data we look at\n",
    "max_j = 11964000 # pre determined number of rows of the file\n",
    "# The stepsize\n",
    "STEPSIZE = int((1-RHO)*H)\n",
    "# Set the lag once\n",
    "LAG = 20\n",
    "# Set the rank once\n",
    "RANK = 7 # K - 1\n",
    "# Set threshold delta\n",
    "DELTA = 0.0005\n",
    "# Path\n",
    "PATH = \"/Downloads/data.csv\"\n",
    "# Adaptive group lasso solvers\n",
    "group_lasso_prox_rank_b = GroupLassoProxRankSelection()\n",
    "group_lasso_prox_rank_a = GroupLassoProxRankSelection()\n",
    "group_lasso_prox_b = GroupLassoProxLagSelection()\n",
    "group_lasso_prox_a = GroupLassoProxLagSelection()\n",
    "# Define tau rank, tau lag and tau ridge and gamma\n",
    "tau_rank_b = 1e10\n",
    "tau_rank_a = 1e10\n",
    "tau_lag_b = 0.01\n",
    "tau_lag_a = 0.01\n",
    "gamma = 2\n",
    "\n",
    "for j in range(0, max_j, STEPSIZE):\n",
    "    try:\n",
    "        # This should be the path to the data_1.csv file on your local machine\n",
    "        skip_rows = j\n",
    "        n_rows = H\n",
    "\n",
    "        VECM_model_b = VECMModelHD()\n",
    "        VECM_model_a = VECMModelHD()\n",
    "        helper_methods = Helpers()\n",
    "\n",
    "        binance, bitmart, bybit, hashkey, htx, bitget, deribit, kuma, hyperliquid = helper_methods.read_csv_to_dataframe(PATH, skip_rows, n_rows, DF_NAMES)\n",
    "        # We will skip hashkey exchange\n",
    "        Y_b = VECM_model_b.construct_Y([binance.bids, bitmart.bids, bybit.bids, htx.bids, bitget.bids, deribit.bids, kuma.bids, hyperliquid.bids])\n",
    "        Y_a = VECM_model_a.construct_Y([binance.asks, bitmart.asks, bybit.asks, htx.asks, bitget.asks, deribit.asks, kuma.asks, hyperliquid.asks])\n",
    "        # Train test split\n",
    "        Y_train_b, Y_test_b = VECM_model_b.train_test_split_Y(Y_b, RHO)\n",
    "        Y_train_a, Y_test_a = VECM_model_a.train_test_split_Y(Y_a, RHO)\n",
    "        # Set the lag\n",
    "        VECM_model_b.lag = LAG\n",
    "        VECM_model_a.lag = LAG\n",
    "        # Build the VECM matrices\n",
    "        VECM_model_b.build_vecm_matrices(Y_train_b)\n",
    "        VECM_model_a.build_vecm_matrices(Y_train_a)\n",
    "        # Build frisch waugh rank matrices\n",
    "        dY_tilde_b, Y1_tilde_b, Pi_tilde_b = VECM_model_b.frisch_waugh_rank_matrices()\n",
    "        dY_tilde_a, Y1_tilde_a, Pi_tilde_a = VECM_model_a.frisch_waugh_rank_matrices()\n",
    "\n",
    "        # Compute qr decomp\n",
    "        Q_hat_b, R_tilde_b, Perm_b = VECM_model_b.qr_decomp(Pi_tilde_b.T)\n",
    "        Q_hat_a, R_tilde_a, Perm_a = VECM_model_a.qr_decomp(Pi_tilde_a.T)\n",
    "        # Compute permuation matrices\n",
    "        Pmat_b = np.eye(Perm_b.shape[0])[:, Perm_b]\n",
    "        Pmat_a = np.eye(Perm_a.shape[0])[:, Perm_a]\n",
    "        # Compute mu_tildes\n",
    "        mu_tilde_b = VECM_model_b.compute_mu_tilde(R_tilde_b)\n",
    "        mu_tilde_a = VECM_model_a.compute_mu_tilde(R_tilde_a)\n",
    "        # Rank pre estimate\n",
    "        r_hat_qr_decomp_b = VECM_model_b.rank_pre_estimate(mu_tilde_b)\n",
    "        r_hat_qr_decomp_a = VECM_model_a.rank_pre_estimate(mu_tilde_a)\n",
    "        # Betas\n",
    "        beta_b = VECM_model_b.get_beta(Q_hat_b, r_hat_qr_decomp_b)\n",
    "        beta_a = VECM_model_a.get_beta(Q_hat_a, r_hat_qr_decomp_a)\n",
    "        # Regressor matrices\n",
    "        dY_tilde_b = Pmat_b.T @ dY_tilde_b\n",
    "        dY_tilde_a = Pmat_a.T @ dY_tilde_a\n",
    "\n",
    "        Y1_tilde_b = Pmat_b.T @ Y1_tilde_b\n",
    "        Y1_tilde_a = Pmat_a.T @ Y1_tilde_a\n",
    "\n",
    "        T = VECM_model_b.T_eff\n",
    "        K = VECM_model_b.K\n",
    "\n",
    "        weights = group_lasso_prox_rank_b.compute_weights(mu_tilde_b, gamma)\n",
    "        weights = group_lasso_prox_rank_b.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_rank_b.construct_Y_Z_matrices(Q_hat_b, Y1_tilde_b, dY_tilde_b)\n",
    "\n",
    "        R_hat_b, loss_history = group_lasso_prox_rank_b.fit(R_tilde_b, Y, Z, tau_rank_b, weights, tol = 1e-20)\n",
    "\n",
    "        Sigma_w_b = VECM_model_b.residual_covariance(dY_tilde_b, R_hat_b.T @ Q_hat_b.T @ Y1_tilde_b)\n",
    "\n",
    "        group_lasso_prox_rank_a = GroupLassoProxRankSelection()\n",
    "        weights = group_lasso_prox_rank_a.compute_weights(mu_tilde_a, gamma)\n",
    "        weights = group_lasso_prox_rank_a.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_rank_a.construct_Y_Z_matrices(Q_hat_a, Y1_tilde_a, dY_tilde_a)\n",
    "        R_hat_a, loss_history = group_lasso_prox_rank_a.fit(R_tilde_a, Y, Z, tau_rank_a, weights, tol = 1e-20)\n",
    "\n",
    "        Sigma_w_a = VECM_model_a.residual_covariance(dY_tilde_a, R_hat_a.T @ Q_hat_a.T @ Y1_tilde_a)\n",
    "\n",
    "        # Set rank\n",
    "        VECM_model_b.rank = RANK\n",
    "        VECM_model_a.rank = RANK\n",
    "        # Get alphas\n",
    "        alpha_b = VECM_model_b.get_alpha(Pmat_b, R_hat_b, r_hat_qr_decomp_b)\n",
    "        alpha_a = VECM_model_a.get_alpha(Pmat_a, R_hat_a, r_hat_qr_decomp_a)\n",
    "        # Frisch waugh lag matrices\n",
    "        dY_check_b, dX_check_b = VECM_model_b.frisch_waugh_lag_matrices()\n",
    "        dY_check_a, dX_check_a = VECM_model_a.frisch_waugh_lag_matrices()\n",
    "        # Gammas pre estimates\n",
    "        Gamma_check_b = VECM_model_b.least_squares_estimate_Gamma(dX_check_b, dY_check_b)\n",
    "        Gamma_check_a = VECM_model_a.least_squares_estimate_Gamma(dX_check_a, dY_check_a)\n",
    "        c_b = 1\n",
    "        c_a = 1\n",
    "        tau_ridge_b = c_b * np.log(T)\n",
    "        tau_ridge_a = c_a * np.log(T)\n",
    "        Gamma_tilde_b = VECM_model_b.ridge_estimator_Gamma(dX_check_b, dY_check_b, tau_ridge_b, LAG)\n",
    "        Gamma_tilde_a = VECM_model_a.ridge_estimator_Gamma(dX_check_a, dY_check_a, tau_ridge_a, LAG)\n",
    "\n",
    "        # lag estimation\n",
    "        K = VECM_model_b.K\n",
    "        weights = group_lasso_prox_b.compute_weights(Gamma_tilde_b, LAG, K, gamma)\n",
    "        weights = group_lasso_prox_b.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_b.helper_loss(LAG, dY_check_b)\n",
    "        Gamma_hat_b, loss_history = group_lasso_prox_b.fit(Gamma_tilde_b, Y, Z, LAG, K, tau_lag_b, weights)\n",
    "        Sigma_w_b = VECM_model_b.residual_covariance(dY_check_b, Gamma_hat_b @ dX_check_b)\n",
    "\n",
    "\n",
    "        weights = group_lasso_prox_a.compute_weights(Gamma_tilde_a, LAG, K, gamma)\n",
    "        weights = group_lasso_prox_a.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_a.helper_loss(LAG, dY_check_a)\n",
    "        Gamma_hat_a, loss_history = group_lasso_prox_a.fit(Gamma_tilde_a, Y, Z, LAG, K, tau_lag_a, weights)\n",
    "        Sigma_w_a = VECM_model_a.residual_covariance(dY_check_a, Gamma_hat_a @ dX_check_a)\n",
    "\n",
    "        # set gamma and lag\n",
    "        Gammas_b = Gamma_hat_b[:, :VECM_model_b.lag*K]\n",
    "        Gammas_a = Gamma_hat_a[:, :VECM_model_a.lag*K]\n",
    "\n",
    "        VECM_model_b.lag = LAG\n",
    "        VECM_model_a.lag = LAG\n",
    "\n",
    "        # Compute the orthogonal compoents of alpha and beta\n",
    "        alpha_perp_b = VECM_model_b.compute_null_space_basis(alpha_b.T)\n",
    "        beta_perp_b = VECM_model_b.compute_null_space_basis(beta_b.T)\n",
    "        alpha_perp_a = VECM_model_a.compute_null_space_basis(alpha_a.T)\n",
    "        beta_perp_a = VECM_model_a.compute_null_space_basis(beta_a.T)\n",
    "        # First evaluate the latent efficient price from Section 4.2 of our paper\n",
    "        persistent_component_b, transitory_component_b = VECM_model_b.compute_P_T_decomposition(alpha_b, beta_b, alpha_perp_b, beta_perp_b, Y_test_b)\n",
    "        persistent_component_a, transitory_component_a = VECM_model_a.compute_P_T_decomposition(alpha_b, beta_b, alpha_perp_a, beta_perp_a, Y_test_a)\n",
    "\n",
    "\n",
    "        # Loop over all variables to compute the score respectively\n",
    "        for idx, exchange_name in zip(range(len(DF_NAMES_CLEAN)), DF_NAMES_CLEAN):\n",
    "            # Define efficient latent prices\n",
    "            efficient_price_b = persistent_component_b[idx]\n",
    "            efficient_price_a = persistent_component_a[idx]\n",
    "\n",
    "            Y_test_b_idx = Y_test_b[idx]\n",
    "            Y_test_a_idx = Y_test_a[idx]\n",
    "            # Performance for bid series\n",
    "            crossed_below_delta = np.where(Y_test_b_idx < efficient_price_b * (1 - DELTA))[0]\n",
    "            crossed_above = np.where(Y_test_b_idx > efficient_price_b)[0]\n",
    "\n",
    "            crossed_above_delta = np.where(Y_test_b_idx > efficient_price_b * (1 + DELTA))[0]\n",
    "            crossed_below = np.where(Y_test_b_idx < efficient_price_b)[0]\n",
    "\n",
    "            for i_k in crossed_below_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_above[np.where(crossed_above > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_b_idx[i_k_1] - Y_test_b_idx[i_k]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['BID_SIDE']['TP'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FP'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            for i_k in crossed_above_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_below[np.where(crossed_below > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_b_idx[i_k] - Y_test_b_idx[i_k_1]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['BID_SIDE']['TN'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FN'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            # Performance for ask series\n",
    "            crossed_below_delta = np.where(Y_test_a_idx < efficient_price_a * (1 - DELTA))[0]\n",
    "            crossed_above = np.where(Y_test_a_idx > efficient_price_a)[0]\n",
    "\n",
    "            crossed_above_delta = np.where(Y_test_a_idx > efficient_price_a * (1 + DELTA))[0]\n",
    "            crossed_below = np.where(Y_test_a_idx < efficient_price_a)[0]\n",
    "\n",
    "            for i_k in crossed_below_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_above[np.where(crossed_above > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_a_idx[i_k_1] - Y_test_a_idx[i_k]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['TP'] += 1\n",
    "                    elif difference < 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['FP'] += 1\n",
    "                \n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            for i_k in crossed_above_delta:\n",
    "                try:\n",
    "                    i_k_1 = crossed_below[np.where(crossed_below > i_k)[0][0]]\n",
    "\n",
    "                    difference = Y_test_a_idx[i_k] - Y_test_a_idx[i_k_1]\n",
    "                    if difference > 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['TN'] += 1\n",
    "                    elif difference < 0:\n",
    "                        score[exchange_name]['ASK_SIDE']['FN'] += 1\n",
    "\n",
    "                # Breaks when ther is no index left for wich np.where(... > i_k)[0][0] is true\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        print(f\"###{j / max_j}###\")\n",
    "\n",
    "    except Exception as exp:\n",
    "        print(f\"Something went wrong: {exp}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentages of the score\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    accuracy_bid_side_up = score[exchange_name]['BID_SIDE']['TP'] / (score[exchange_name]['BID_SIDE']['TP'] + score[exchange_name]['BID_SIDE']['FP'])\n",
    "    accuracy_ask_side_up = score[exchange_name]['ASK_SIDE']['TP'] / (score[exchange_name]['ASK_SIDE']['TP'] + score[exchange_name]['ASK_SIDE']['FP'])\n",
    "\n",
    "    accuracy_bid_side_down = score[exchange_name]['BID_SIDE']['TN'] / (score[exchange_name]['BID_SIDE']['TN'] + score[exchange_name]['BID_SIDE']['FN'])\n",
    "    accuracy_ask_side_down = score[exchange_name]['ASK_SIDE']['TN'] / (score[exchange_name]['ASK_SIDE']['TN'] + score[exchange_name]['ASK_SIDE']['FN'])\n",
    "\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_bid_side_up} for updwards correction on the bid side\")\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_ask_side_up} for downwards correction on the ask side\")\n",
    "\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_bid_side_down} for updwards correction on the bid side\")\n",
    "    print(f\"{exchange_name} has accuracy of {accuracy_ask_side_down} for downwards correction on the ask side\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbe5cf",
   "metadata": {},
   "source": [
    "## Using a Multionmial Logit Model (Section 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exchange names as hinted in footnote 1 of our paper\n",
    "DF_NAMES = ['binance', 'bitmart', 'bybit', 'hashkey', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "DF_NAMES_CLEAN = ['binance', 'bitmart', 'bybit', 'htx', 'bitget', 'deribit', 'kuma', 'hyperliquid']\n",
    "\n",
    "# Keep track of the performance of every exchange respectively\n",
    "score = {}\n",
    "for exchange_name in DF_NAMES_CLEAN:\n",
    "    score[exchange_name] = {}\n",
    "    score[exchange_name]['BID_SIDE'] = {}\n",
    "    score[exchange_name]['BID_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['BID_SIDE']['FN'] = 0\n",
    "\n",
    "    score[exchange_name]['ASK_SIDE'] = {}\n",
    "    score[exchange_name]['ASK_SIDE']['TP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FP'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['TN'] = 0\n",
    "    score[exchange_name]['ASK_SIDE']['FN'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfcc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size\n",
    "H = 50_000\n",
    "# Train percentage\n",
    "RHO = 0.7\n",
    "# Max total length of the data we look at\n",
    "max_j = 1_000_000 # pre determined number of rows of the file\n",
    "# The stepsize\n",
    "STEPSIZE = int((1-RHO)*H)\n",
    "# Set the lag once\n",
    "LAG = 20\n",
    "# Set the rank once\n",
    "RANK = 7 # K - 1\n",
    "# Set threshold delta\n",
    "DELTA = 0.0005\n",
    "# Path\n",
    "PATH = \"/Downloads/data.csv\"\n",
    "# Adaptive group lasso solvers\n",
    "group_lasso_prox_rank_b = GroupLassoProxRankSelection()\n",
    "group_lasso_prox_rank_a = GroupLassoProxRankSelection()\n",
    "group_lasso_prox_b = GroupLassoProxLagSelection()\n",
    "group_lasso_prox_a = GroupLassoProxLagSelection()\n",
    "# Define tau rank, tau lag and tau ridge and gamma\n",
    "tau_rank_b = 1e10\n",
    "tau_rank_a = 1e10\n",
    "tau_lag_b = 0.01\n",
    "tau_lag_a = 0.01\n",
    "gamma = 2\n",
    "\n",
    "# Define lookback of clusters\n",
    "D = 20\n",
    "kappa = 0.7\n",
    "\n",
    "for j in range(0, max_j, STEPSIZE):\n",
    "    try:\n",
    "        # This should be the path to the data_1.csv file on your local machine\n",
    "        skip_rows = j\n",
    "        n_rows = H\n",
    "\n",
    "        VECM_model_b = VECMModelHD()\n",
    "        VECM_model_a = VECMModelHD()\n",
    "        helper_methods = Helpers()\n",
    "\n",
    "        binance, bitmart, bybit, hashkey, htx, bitget, deribit, kuma, hyperliquid = helper_methods.read_csv_to_dataframe(PATH, skip_rows, n_rows, DF_NAMES)\n",
    "        # We will skip hashkey exchange\n",
    "        Y_b = VECM_model_b.construct_Y([binance.bids, bitmart.bids, bybit.bids, htx.bids, bitget.bids, deribit.bids, kuma.bids, hyperliquid.bids])\n",
    "        Y_a = VECM_model_a.construct_Y([binance.asks, bitmart.asks, bybit.asks, htx.asks, bitget.asks, deribit.asks, kuma.asks, hyperliquid.asks])\n",
    "        # Train test split\n",
    "        Y_train_b, Y_test_b = VECM_model_b.train_test_split_Y(Y_b, RHO)\n",
    "        Y_train_a, Y_test_a = VECM_model_a.train_test_split_Y(Y_a, RHO)\n",
    "        # Set the lag\n",
    "        VECM_model_b.lag = LAG\n",
    "        VECM_model_a.lag = LAG\n",
    "        # Build the VECM matrices\n",
    "        VECM_model_b.build_vecm_matrices(Y_train_b)\n",
    "        VECM_model_a.build_vecm_matrices(Y_train_a)\n",
    "        # Build frisch waugh rank matrices\n",
    "        dY_tilde_b, Y1_tilde_b, Pi_tilde_b = VECM_model_b.frisch_waugh_rank_matrices()\n",
    "        dY_tilde_a, Y1_tilde_a, Pi_tilde_a = VECM_model_a.frisch_waugh_rank_matrices()\n",
    "\n",
    "        # Compute qr decomp\n",
    "        Q_hat_b, R_tilde_b, Perm_b = VECM_model_b.qr_decomp(Pi_tilde_b.T)\n",
    "        Q_hat_a, R_tilde_a, Perm_a = VECM_model_a.qr_decomp(Pi_tilde_a.T)\n",
    "        # Compute permuation matrices\n",
    "        Pmat_b = np.eye(Perm_b.shape[0])[:, Perm_b]\n",
    "        Pmat_a = np.eye(Perm_a.shape[0])[:, Perm_a]\n",
    "        # Compute mu_tildes\n",
    "        mu_tilde_b = VECM_model_b.compute_mu_tilde(R_tilde_b)\n",
    "        mu_tilde_a = VECM_model_a.compute_mu_tilde(R_tilde_a)\n",
    "        # Rank pre estimate\n",
    "        r_hat_qr_decomp_b = VECM_model_b.rank_pre_estimate(mu_tilde_b)\n",
    "        r_hat_qr_decomp_a = VECM_model_a.rank_pre_estimate(mu_tilde_a)\n",
    "        # Betas\n",
    "        beta_b = VECM_model_b.get_beta(Q_hat_b, r_hat_qr_decomp_b)\n",
    "        beta_a = VECM_model_a.get_beta(Q_hat_a, r_hat_qr_decomp_a)\n",
    "        # Regressor matrices\n",
    "        dY_tilde_b = Pmat_b.T @ dY_tilde_b\n",
    "        dY_tilde_a = Pmat_a.T @ dY_tilde_a\n",
    "\n",
    "        Y1_tilde_b = Pmat_b.T @ Y1_tilde_b\n",
    "        Y1_tilde_a = Pmat_a.T @ Y1_tilde_a\n",
    "\n",
    "        T = VECM_model_b.T_eff\n",
    "        K = VECM_model_b.K\n",
    "\n",
    "        weights = group_lasso_prox_rank_b.compute_weights(mu_tilde_b, gamma)\n",
    "        weights = group_lasso_prox_rank_b.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_rank_b.construct_Y_Z_matrices(Q_hat_b, Y1_tilde_b, dY_tilde_b)\n",
    "\n",
    "        R_hat_b, loss_history = group_lasso_prox_rank_b.fit(R_tilde_b, Y, Z, tau_rank_b, weights, tol = 1e-20)\n",
    "\n",
    "        Sigma_w_b = VECM_model_b.residual_covariance(dY_tilde_b, R_hat_b.T @ Q_hat_b.T @ Y1_tilde_b)\n",
    "\n",
    "        group_lasso_prox_rank_a = GroupLassoProxRankSelection()\n",
    "        weights = group_lasso_prox_rank_a.compute_weights(mu_tilde_a, gamma)\n",
    "        weights = group_lasso_prox_rank_a.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_rank_a.construct_Y_Z_matrices(Q_hat_a, Y1_tilde_a, dY_tilde_a)\n",
    "        R_hat_a, loss_history = group_lasso_prox_rank_a.fit(R_tilde_a, Y, Z, tau_rank_a, weights, tol = 1e-20)\n",
    "\n",
    "        Sigma_w_a = VECM_model_a.residual_covariance(dY_tilde_a, R_hat_a.T @ Q_hat_a.T @ Y1_tilde_a)\n",
    "\n",
    "        # Set rank\n",
    "        VECM_model_b.rank = RANK\n",
    "        VECM_model_a.rank = RANK\n",
    "        # Get alphas\n",
    "        alpha_b = VECM_model_b.get_alpha(Pmat_b, R_hat_b, r_hat_qr_decomp_b)\n",
    "        alpha_a = VECM_model_a.get_alpha(Pmat_a, R_hat_a, r_hat_qr_decomp_a)\n",
    "        # Frisch waugh lag matrices\n",
    "        dY_check_b, dX_check_b = VECM_model_b.frisch_waugh_lag_matrices()\n",
    "        dY_check_a, dX_check_a = VECM_model_a.frisch_waugh_lag_matrices()\n",
    "        # Gammas pre estimates\n",
    "        Gamma_check_b = VECM_model_b.least_squares_estimate_Gamma(dX_check_b, dY_check_b)\n",
    "        Gamma_check_a = VECM_model_a.least_squares_estimate_Gamma(dX_check_a, dY_check_a)\n",
    "        c_b = 1\n",
    "        c_a = 1\n",
    "        tau_ridge_b = c_b * np.log(T)\n",
    "        tau_ridge_a = c_a * np.log(T)\n",
    "        Gamma_tilde_b = VECM_model_b.ridge_estimator_Gamma(dX_check_b, dY_check_b, tau_ridge_b, LAG)\n",
    "        Gamma_tilde_a = VECM_model_a.ridge_estimator_Gamma(dX_check_a, dY_check_a, tau_ridge_a, LAG)\n",
    "\n",
    "        # lag estimation\n",
    "        K = VECM_model_b.K\n",
    "        weights = group_lasso_prox_b.compute_weights(Gamma_tilde_b, LAG, K, gamma)\n",
    "        weights = group_lasso_prox_b.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_b.helper_loss(LAG, dY_check_b)\n",
    "        Gamma_hat_b, loss_history = group_lasso_prox_b.fit(Gamma_tilde_b, Y, Z, LAG, K, tau_lag_b, weights)\n",
    "        Sigma_w_b = VECM_model_b.residual_covariance(dY_check_b, Gamma_hat_b @ dX_check_b)\n",
    "\n",
    "\n",
    "        weights = group_lasso_prox_a.compute_weights(Gamma_tilde_a, LAG, K, gamma)\n",
    "        weights = group_lasso_prox_a.scale_weights(weights)\n",
    "        Y, Z = group_lasso_prox_a.helper_loss(LAG, dY_check_a)\n",
    "        Gamma_hat_a, loss_history = group_lasso_prox_a.fit(Gamma_tilde_a, Y, Z, LAG, K, tau_lag_a, weights)\n",
    "        Sigma_w_a = VECM_model_a.residual_covariance(dY_check_a, Gamma_hat_a @ dX_check_a)\n",
    "\n",
    "        # set gamma and lag\n",
    "        Gammas_b = Gamma_hat_b[:, :VECM_model_b.lag*K]\n",
    "        Gammas_a = Gamma_hat_a[:, :VECM_model_a.lag*K]\n",
    "\n",
    "        VECM_model_b.lag = LAG\n",
    "        VECM_model_a.lag = LAG\n",
    "\n",
    "        # Compute the orthogonal compoents of alpha and beta\n",
    "        alpha_perp_b = VECM_model_b.compute_null_space_basis(alpha_b.T)\n",
    "        beta_perp_b = VECM_model_b.compute_null_space_basis(beta_b.T)\n",
    "        alpha_perp_a = VECM_model_a.compute_null_space_basis(alpha_a.T)\n",
    "        beta_perp_a = VECM_model_a.compute_null_space_basis(beta_a.T)\n",
    "        # First evaluate the latent efficient price from Section 4.2 of our paper\n",
    "        persistent_component_b_train, transitory_component_b_train = VECM_model_b.compute_P_T_decomposition(alpha_b, beta_b, alpha_perp_b, beta_perp_b, Y_train_b)\n",
    "        persistent_component_a_train, transitory_component_a_train = VECM_model_a.compute_P_T_decomposition(alpha_b, beta_b, alpha_perp_a, beta_perp_a, Y_train_a)\n",
    "\n",
    "        persistent_component_b_test, transitory_component_b_test = VECM_model_b.compute_P_T_decomposition(alpha_b, beta_b, alpha_perp_b, beta_perp_b, Y_test_b)\n",
    "        persistent_component_a_test, transitory_component_a_test = VECM_model_a.compute_P_T_decomposition(alpha_b, beta_b, alpha_perp_a, beta_perp_a, Y_test_a)\n",
    "\n",
    "\n",
    "        # Loop over all variables to compute the score respectively\n",
    "        for idx, exchange_name in zip(range(len(DF_NAMES_CLEAN)), DF_NAMES_CLEAN):\n",
    "            # Only test bid series here\n",
    "            multionmial_logit_b = MultinomialLogit()\n",
    "            multionmial_logit_a = MultinomialLogit()\n",
    "\n",
    "            # Define leader and lagger series\n",
    "            # x is leading y\n",
    "            # Define train data\n",
    "            x_series_train_b = pd.Series((persistent_component_b_train)[idx])\n",
    "            y_series_train_b = pd.Series(Y_train_b[idx])\n",
    "\n",
    "            x_series_train_a = pd.Series((persistent_component_a_train)[idx])\n",
    "            y_series_train_a = pd.Series(Y_train_a[idx])\n",
    "\n",
    "            # Define test data\n",
    "            x_series_test_b = pd.Series(persistent_component_b_test[idx])\n",
    "            y_series_test_b = pd.Series(Y_test_b[idx])\n",
    "\n",
    "            x_series_test_a = pd.Series(persistent_component_a_test[idx])\n",
    "            y_series_test_a = pd.Series(Y_test_a[idx])\n",
    "\n",
    "            # Get the data frame with cluster ids\n",
    "            cluster_df_train_b = multionmial_logit_b.compute_static_cluster_returns(x_series_train_b, y_series_train_b)\n",
    "            cluster_df_test_b = multionmial_logit_b.compute_static_cluster_returns(x_series_test_b, y_series_test_b)\n",
    "\n",
    "            cluster_df_train_a = multionmial_logit_a.compute_static_cluster_returns(x_series_train_a, y_series_train_a)\n",
    "            cluster_df_test_a = multionmial_logit_a.compute_static_cluster_returns(x_series_test_a, y_series_test_a)\n",
    "\n",
    "            # Generate the training data and fit the model with the train data we already have\n",
    "            # Lookback length for clusters\n",
    "            X_train_b, y_train_b = multionmial_logit_b.generate_model_data(cluster_df_train_b, D)\n",
    "            X_test_b, y_test_b = multionmial_logit_b.generate_model_data(cluster_df_test_b, D)\n",
    "\n",
    "            X_train_a, y_train_a = multionmial_logit_a.generate_model_data(cluster_df_train_a, D)\n",
    "            X_test_a, y_test_a = multionmial_logit_a.generate_model_data(cluster_df_test_a, D)\n",
    "\n",
    "            # Fit models\n",
    "            model_b = multionmial_logit_b.fit(X_train_b, y_train_b)\n",
    "            model_a = multionmial_logit_a.fit(X_train_a, y_train_a)\n",
    "            \n",
    "            # Check the performance\n",
    "\n",
    "            for i in range(len(X_test_b)):\n",
    "                prediction = model_b.predict(X_test_b[i])[0]\n",
    "                true_value = np.sign(y_test_b[i])\n",
    "\n",
    "                # down prediction\n",
    "            \n",
    "                # up prediction\n",
    "                if np.max(prediction) == prediction[2] and np.max(prediction) >= kappa:\n",
    "                    if true_value == +1:\n",
    "                        score[exchange_name]['BID_SIDE']['TP'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FP'] += 1\n",
    "\n",
    "                elif np.max(prediction) == prediction[0] and np.max(prediction) >= kappa:\n",
    "                    if true_value == -1:\n",
    "                        score[exchange_name]['BID_SIDE']['TN'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FN'] += 1\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            for i in range(len(X_test_a)):\n",
    "                prediction = model_a.predict(X_test_a[i])[0]\n",
    "                true_value = np.sign(y_test_a[i])\n",
    "\n",
    "                # down prediction\n",
    "            \n",
    "                # up prediction\n",
    "                if np.max(prediction) == prediction[2] and np.max(prediction) >= kappa:\n",
    "                    if true_value == +1:\n",
    "                        score[exchange_name]['ASK_SIDE']['TP'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['ASK_SIDE']['FP'] += 1\n",
    "\n",
    "                elif np.max(prediction) == prediction[0] and np.max(prediction) >= kappa:\n",
    "                    if true_value == -1:\n",
    "                        score[exchange_name]['BID_SIDE']['TN'] += 1\n",
    "                    else:\n",
    "                        score[exchange_name]['BID_SIDE']['FN'] += 1\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        print(f\"###{j / max_j}###\")\n",
    "\n",
    "    except Exception as exp:\n",
    "        # ignore warnings, sometimes labels are only +1 and -1 due to the data being poorly scaled\n",
    "        # need to optimize a bit here\n",
    "        print(f\"Something went wrong: {exp}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3246a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "score['kuma']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
